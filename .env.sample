# .env.example
# Copy this file to .env and fill in your specific values.
# This file should NOT be committed to Git.

# --- WORKSPACE CONFIGURATION ---
# Defines the absolute path to the folder on your host machine that you want
# to make available to the container. This folder will appear as /workspace inside Docker.
# Example (Linux/macOS): AI_ASSISTANT_WORKSPACE=/home/user/projects
# Example (Windows/WSL): AI_ASSISTANT_WORKSPACE=/c/Users/User/Documents/projects
AI_ASSISTANT_WORKSPACE=

# --- CLOUD PROVIDER API KEYS ---
# Fill in only for the providers you intend to use.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIza..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- GLOBAL DEFAULT CONFIGURATION (LEVEL 3 - LOWEST PRIORITY) ---
# Defines the provider and model to use if no more specific configuration is found.
# Valid Providers: OLLAMA, OPENAI, GEMINI, ANTHROPIC
DEFAULT_PROVIDER=OLLAMA
DEFAULT_MODEL_NAME=llama3:8b

# --- OPTIONAL TASK-SPECIFIC OVERRIDES (LEVEL 2 - MEDIUM PRIORITY) ---
# If these variables are set, they will take priority over the global default for their respective tasks.
# Use the format PROVIDER:MODEL_NAME.
# Leave blank or comment out to use the global default.
#
# PLANNING_MODEL_IDENTIFIER=ANTHROPIC:claude-3-sonnet-20240229
# DOCS_MODEL_IDENTIFIER=OPENAI:gpt-4o
# ANALYSIS_MODEL_IDENTIFIER=OLLAMA:codegemma
# EDITING_MODEL_IDENTIFIER=OLLAMA:deepseek-coder-v2

# --- OLLAMA CONFIGURATION ---
# The URL where the Ollama service is listening.
# http://host.docker.internal:11434 is recommended for Docker Desktop (Mac, Windows).
# For Linux, you may need to use your machine's IP address.
OLLAMA_BASE_URL=http://host.docker.internal:11434
