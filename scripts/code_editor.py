# scripts/code_editor.py
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import json

def run_code_editing(instruction: str, model_name: str, ollama_base_url: str) -> dict:
    # In a real-world scenario, this function would also need the context of the files.
    # For this API example, we will simplify and just generate the editing plan.
    
    llm = ChatOllama(model=model_name, temperature=0, base_url=ollama_base_url)
    prompt = ChatPromptTemplate.from_template("""
    You are an experienced software engineer. Your task is to create an editing plan for one or more code files based on a request.
    Respond ONLY with a JSON code block containing a list of operations.

    **Request:** "{instruction}"
    
    Example Response:
    ```json
    {{
      "plan": [
        {{
          "file_path": "src/user.py",
          "action": "replace_block",
          "comment": "Adds the email field to the User model.",
          "start_line": 10,
          "end_line": 15,
          "new_code": "class User:\\n    id: int\\n    name: str\\n    email: str"
        }}
      ]
    }}
    ```
    """)
    chain = prompt | llm | StrOutputParser()
    response_str = chain.invoke({"instruction": instruction})

    try:
        # Attempt to parse the JSON generated by the LLM
        return json.loads(response_str)
    except json.JSONDecodeError:
        return {"error": "LLM did not return a valid JSON.", "raw_response": response_str}
